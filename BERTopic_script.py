# -*- coding: utf-8 -*-
"""BERTopic Script

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17_oGEy7ay9p7UJrxTYyk-IApuyD2JE9P

# BERTopic Script

This script was developped in order to compare the effectiveness of possible topic modelling approaches.

This script performs generational topic modelling on the dataset of processed love song lyrics using BERTopic.

The workflow includes:
- Text preprocessing (lemmatisation via SpaCy).
- Generational categorisation based on release year.
- Topic modelling using BERTopic with UMAP dimensionality reduction to extract meaningful topics.
- Evaluation of topic coherence using Gensim’s CoherenceModel (to assess interpretability).

Although this BERTopic model was implemented successfully, its results were not analysed in depth, as LDA outperformed it in comparative evaluation. This code is included here for transparency and reproducibility.
"""

# Required packages
!pip install bertopic sentence-transformers umap-learn hdbscan spacy nltk
!python -m spacy download en_core_web_sm
!pip install memory_profiler
!pip install gensim

# Connect your drive to access the dataset
# (make sure to upload the datset to your drive)
from google.colab import drive
drive.mount('/content/drive')

# --- Imports ---
import pandas as pd
import numpy as np
import random
import spacy
import nltk
from tqdm import tqdm
from bertopic import BERTopic
from umap import UMAP
from gensim.corpora import Dictionary
from gensim.models.coherencemodel import CoherenceModel
import warnings

warnings.filterwarnings("ignore")

# --- Step 1: Setup ---

# Set random seed for reproducibility
SEED = 42
np.random.seed(SEED)
random.seed(SEED)

# Load SpaCy model
import spacy
nlp = spacy.load("en_core_web_sm", disable=["parser", "ner"])

# Lemmatisation function
# Lemmatises the list of tokenised songs, keeping specified POS tags.
def lemmatise(tokens_list):
    results = []
    allowed_postags = ['NOUN', 'ADJ', 'VERB', 'ADV']
    for tokens in tqdm(tokens_list, desc="Lemmatising documents"):
        if not isinstance(tokens, list) or not all(isinstance(word, str) for word in tokens):
            results.append([])
            continue
        doc = nlp(" ".join(tokens))
        results.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])
    return results

# This function assigns generational tags
def assign_generation(year):
    try:
        year = int(float(year))  # handles strings or floats like "1998.0"
        if 1965 <= year <= 1980:
            return "Gen X"
        elif 1981 <= year <= 2000:
            return "Millennial"
        elif 2001 <= year <= 2010:
            return "Gen Z"
        elif year >= 2011:
            return "Gen Alpha"
        else:
            return "Other"
    except:
        return "Unknown"


# --- Step 2: Main Pipeline ---
FILE_PATH = "/content/drive/MyDrive/processed_love_song_dataset_FINAL.csv"

try:
    df = pd.read_csv(FILE_PATH)
    print(f"Loaded dataset from: {FILE_PATH}")
except Exception as e:
    raise FileNotFoundError(f"Failed to load file: {e}")

# Validate Required Columns
required_cols = ['Processed_Tokens', 'Release Year']
if not all(col in df.columns for col in required_cols):
    raise ValueError("Missing required columns in dataset.")

# Apply Generation Labels
df['Generation'] = df['Release Year'].apply(assign_generation)

# Convert Processed_Tokens (string) to lists ---
import ast

def safe_literal_eval(val):
    try:
        return ast.literal_eval(val) if isinstance(val, str) else []
    except (ValueError, SyntaxError):
        return []

df['Preprocessed_Lyrics'] = df['Processed_Tokens'].apply(safe_literal_eval)

# --- Lemmatise ---
df['Lemmatised'] = lemmatise(df['Preprocessed_Lyrics'])
df = df[df['Lemmatised'].str.len() > 0].copy()

# Setup BERTopic
umap_model = UMAP(n_neighbors=10, n_components=3, metric='cosine', low_memory=True, random_state=SEED)

topic_model = BERTopic(
    umap_model=umap_model,
    calculate_probabilities=False,
    verbose=True
)

# --- Step 3: Run BERTopic by Generation ---
generations = df['Generation'].unique()

for gen in sorted(generations):
    if gen in ['Other', 'Unknown']:
        print(f"\n⏭ Skipping Generation: {gen}")
        continue

    print(f"\n--- Processing {gen} ---")
    gen_df = df[df['Generation'] == gen].copy()
    docs = [" ".join(tokens) for tokens in gen_df['Lemmatised'].tolist()]

    if not docs:
        print(f"No documents to process for {gen}. Skipping.")
        continue

    try:
        topics, probs = topic_model.fit_transform(docs)
        print(f"BERTopic model trained for {gen}")

        # Topic Summary
        topic_info = topic_model.get_topic_info()
        valid_topics = topic_info[topic_info['Topic'] != -1]
        print(f"Topics found: {len(valid_topics)}")
        print(valid_topics[['Topic', 'Count', 'Name']].head(5))

        # Coherence Calculation
        topic_ids = valid_topics['Topic'].tolist()
        topic_words = [ [word for word, _ in topic_model.get_topic(tid)] for tid in topic_ids ]
        tokenized_docs = gen_df['Lemmatised'].tolist()
        dictionary = Dictionary(tokenized_docs)

        if topic_words and tokenized_docs:
            coherence_model = CoherenceModel(
                topics=topic_words,
                texts=tokenized_docs,
                dictionary=dictionary,
                coherence='c_v'
            )
            coherence_score = coherence_model.get_coherence()
            print(f"Coherence Score (c_v) for {gen}: {coherence_score:.4f}")
        else:
            print(f"Not enough data for coherence score for {gen}.")

    except Exception as e:
        print(f"Error processing {gen}: {e}")
        continue

print("\n All generations processed.")

